{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TODO: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import google.generativeai as genai\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "import glob\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the mini-eval directory with the 'answers' (LLM-based ground truth) and 'documents' (perturbed documents without tags).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'perturbed_legal_documents'\n",
    "PERTURBATION_TYPES = ['ambiguity', 'inconsistencies', 'misaligned_terminalogy', 'omission', 'structural_flaws']\n",
    "CATEGORIES = ['inText', 'legal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS = [\n",
    "    \"AIzaSyCKtZRj1pJMu1JVO7siNYcqG15oTgPSj3k\", # Aditya\n",
    "    \"AIzaSyDgafwAgDi2Zjvu6jdt_SIZ60VgK1Na32E\", # Aditya\n",
    "    \"AIzaSyCWI7QJXWYBGGWGdL37W8ll0sDIwz0zqlo\", # Aditya\n",
    "    \"AIzaSyCVjSqp_8WwJMVaIi3dVSQDRic5I1869kE\", # Foo\n",
    "    \"AIzaSyCKtZRj1pJMu1JVO7siNYcqG15oTgPSj3k\", # Foo\n",
    "    \"AIzaSyAjby-dj9aBsolOdTDpvU7_x5uje8l4yiQ\", # Foo\n",
    "    \"AIzaSyCN-EJ7s6CIeEybjT3tM_zN0-4xx4Rcqqw\", # Foo\n",
    "    \"AIzaSyCKWwXUILaUvHkyppqY87-cqBad16vZb00\", # Foo\n",
    "    \"AIzaSyCfYpaD89nvVJ6GIitszeWI0KXdlgEAv-Q\", # Foo\n",
    "    \"AIzaSyCsA0PVE_BygEVMdrGs7Upyo4nBk2FTbhM\", # Foo\n",
    "    \"AIzaSyAcqO6uxgeIP5qyxcDZLAY2TC9xyTlBmC0\", # Foo\n",
    "    \"AIzaSyC_86XS-IZzhdfmhBSThwQoYMoQuFeY4mQ\", # Foo\n",
    "    \"AIzaSyAH4zpotMPNF-GlGYmMMAi6ZoCte5b95Hk\", # Ezra\n",
    "    \"AIzaSyDSG4tUWCN6oA7b2XMS8zLOfXG7R987D2Y\", # Ezra\n",
    "    \"AIzaSyDwBOvWeSweppAjbU3fwWqBm0a_M7JGOWw\", # Ezra\n",
    "    \"AIzaSyCqqBjoa2M6HF7aEagzJn_2ckEYrW1s7wY\", # Ezra\n",
    "    \"AIzaSyAGHtD2RAI1geToBsVjk-mIzVeuhlZQtA4\", # Noel\n",
    "    \"AIzaSyBTYgTD42xCABfJy1jsHchkZEhFaw8X1_c\", # Mannan\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Dataset(ABC):\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "\n",
    "class MiniEvalDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.mini_eval_dir = \"mini-eval\"\n",
    "\n",
    "        self.mini_eval_answers_dir = os.path.join(self.mini_eval_dir, \"answers_v1\") # change this later\n",
    "\n",
    "        self.mini_eval_documents_dir = os.path.join(self.mini_eval_dir, \"documents_v1\") # change this later\n",
    "\n",
    "        self.files = [\n",
    "\n",
    "            os.path.relpath(\n",
    "                os.path.join(root, file), self.mini_eval_answers_dir\n",
    "            ).replace(\".json\", \"\")\n",
    "\n",
    "            for root, _, files in os.walk(self.mini_eval_answers_dir)\n",
    "            for file in files\n",
    "\n",
    "        ]\n",
    "        self.files.sort()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.files)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        with open(\n",
    "            os.path.join(self.mini_eval_answers_dir, self.files[idx] + \".json\"),\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as f:\n",
    "\n",
    "            answers = \"\\n\".join(f.readlines())\n",
    "\n",
    "            answers = self.__remove_non_ascii(answers)\n",
    "\n",
    "            answers = json.loads(answers)\n",
    "\n",
    "\n",
    "        with open(\n",
    "            os.path.join(self.mini_eval_documents_dir, self.files[idx] + \".txt\"),\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as f:\n",
    "\n",
    "            documents = \"\\n\".join(f.readlines())\n",
    "\n",
    "            documents = self.__remove_non_ascii(documents)\n",
    "\n",
    "\n",
    "        return {\n",
    "\n",
    "            \"file_name\": self.files[idx],\n",
    "\n",
    "            \"answers\": answers,\n",
    "\n",
    "            \"documents\": documents,\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    def __remove_non_ascii(self, s):\n",
    "\n",
    "        return \"\".join(filter(lambda x: ord(x) < 128, s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    @abstractmethod\n",
    "    def generate(self, prompt):\n",
    "        pass\n",
    "\n",
    "\n",
    "# class GeminiModel(Model):\n",
    "#     def __init__(self):\n",
    "#         self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "#     def generate(self, prompt):\n",
    "#         response = self.model.generate_content(prompt)\n",
    "#         return response.to_dict()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "    \n",
    "# New version with API key cycling\n",
    "class GeminiModel(Model):\n",
    "    def __init__(self, api_keys):\n",
    "        self.api_keys = api_keys\n",
    "        self.key_index = 0\n",
    "        self._set_key(self.api_keys[self.key_index])\n",
    "    \n",
    "    def _set_key(self, key):\n",
    "        os.environ[\"GOOGLE_API_KEY\"] = key\n",
    "        genai.configure(api_key=key)\n",
    "        self.model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "    def generate(self, prompt, max_retries=5):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.model.generate_content(prompt)\n",
    "                return response.to_dict()[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "            except ResourceExhausted:\n",
    "                print(f\"⚠️ API key {self.api_keys[self.key_index]} exhausted. Switching...\")\n",
    "                self.key_index = (self.key_index + 1) % len(self.api_keys)\n",
    "                self._set_key(self.api_keys[self.key_index])\n",
    "        print(\"❌ All keys exhausted or failed.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Methods\n",
    "These ones take in a base model and does some prompting stuff with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfVerificationModel(Model):\n",
    "    def __init__(self, model: Model):\n",
    "        self.model = model\n",
    "\n",
    "    def generate(self, prompt):\n",
    "\n",
    "        failed = True\n",
    "\n",
    "        while failed:\n",
    "            print(\"💡 Asking questions\")\n",
    "            response = self.model.generate(prompt)\n",
    "            is_model_sure_response = self.model.generate(\n",
    "                f\"You are a grader. Verify if the following response to the question is correct. If the answer is correct, say yes. Otherwise, say no.\\nQuestion: {prompt}\\nAnswer: {response}\"\n",
    "            )\n",
    "\n",
    "            print(\"🤖 Model response:\", response)\n",
    "            print(\"🤓 Model sure response:\", is_model_sure_response)\n",
    "\n",
    "            if \"yes\" in is_model_sure_response.lower():\n",
    "                print(\"✅ Model is sure about the answer.\")\n",
    "                failed = False\n",
    "            else:\n",
    "                print(\"❌ Model is not sure. Retrying...\")\n",
    "\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You retrieve elements in each dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MiniEvalDataset()\n",
    "# display(dataset[0][\"answers\"], dataset[0][\"documents\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You check the length like this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dataset)\n",
    "# print(dataset[5][\"file_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_parse_model_response(raw_response):\n",
    "    raw_response = raw_response.strip().strip(\"`\")\n",
    "    if raw_response.startswith(\"json\"):\n",
    "        raw_response = raw_response[4:].strip()\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(raw_response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Failed to parse JSON:\", e)\n",
    "        return None\n",
    "\n",
    "    return parsed\n",
    "\n",
    "\n",
    "def add_section_identified_flag(predictions, ground_truth_perturbations):\n",
    "    gt_locations = {p[\"location\"].strip() for p in ground_truth_perturbations}\n",
    "    gt_changed_texts = [p[\"changed_text\"] for p in ground_truth_perturbations]\n",
    "\n",
    "    for pred in predictions:\n",
    "        # LOCATION MATCH\n",
    "        pred_loc = pred.get(\"location\", \"\").strip()\n",
    "        pred[\"location_match\"] = pred_loc in gt_locations\n",
    "\n",
    "        # TEXT MATCH (check if model's reponse for 'section' matches what was perturbed)\n",
    "        pred_section = pred.get(\"section\", \"\").strip()\n",
    "        pred[\"text_match\"] = any(pred_section in gt_text or gt_text in pred_section for gt_text in gt_changed_texts)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of `generate_responses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(model, dataset, prompt: str, output_dir, num_responses: int = 1):\n",
    "    for sample in tqdm.tqdm(dataset, desc=\"Processing samples\"):\n",
    "        # Prepare base directory and document text\n",
    "        base_name = sample[\"file_name\"]\n",
    "        document_with_tags = sample[\"documents\"]\n",
    "        document_with_tags_removed = sample[\"documents\"].replace(\"<*$p$*>\", \"\") \n",
    "        ground_truth = sample[\"answers\"][0][\"perturbation\"]\n",
    "\n",
    "        for i in range(num_responses):\n",
    "            # Construct output path: outputs/self_consistency/<subdir>/<filename>_i.json\n",
    "            subdir = os.path.join(output_dir, \"self_consistency\", os.path.dirname(base_name))\n",
    "            os.makedirs(subdir, exist_ok=True)\n",
    "            output_path = os.path.join(subdir, os.path.basename(base_name) + f\"_{i}.json\")\n",
    "\n",
    "            # Skip if file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                continue\n",
    "\n",
    "            # Generate model response\n",
    "            model_response = model.generate(\n",
    "                # prompt.replace(\"[DOCUMENT]\", document_with_tags_removed)\n",
    "                prompt.replace(\"[DOCUMENT]\", document_with_tags)\n",
    "            )\n",
    "            parsed_response = clean_and_parse_model_response(model_response)\n",
    "\n",
    "            if parsed_response:\n",
    "                updated_predictions = add_section_identified_flag(parsed_response, ground_truth)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(updated_predictions, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of `explanation_match`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 1 - With LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def explanation_match(evaluation_model: Model, dataset, responses_dir):\n",
    "#     for sample in tqdm.tqdm(dataset, desc=\"Evaluating explanations\"):\n",
    "#         file_name = sample[\"file_name\"]\n",
    "        \n",
    "#         # Normalize and split into subdir + base filename (fixes Windows paths)\n",
    "#         normalized_path = os.path.normpath(file_name)\n",
    "#         subdir = os.path.dirname(normalized_path).replace(\"\\\\\", \"/\")\n",
    "#         base_filename = os.path.basename(normalized_path).replace(\".json\", \"\")\n",
    "\n",
    "#         # Match all _i.json variant files for this sample\n",
    "#         pattern = os.path.join(responses_dir, \"self_consistency\", subdir, f\"{base_filename}_*.json\")\n",
    "#         response_paths = sorted(glob.glob(pattern))\n",
    "\n",
    "#         if not response_paths:\n",
    "#             print(f\"❌ No response files found for: {file_name}\")\n",
    "#             continue\n",
    "\n",
    "#         # Extract GT explanations\n",
    "#         gt_explanations = [\n",
    "#             p[\"explanation\"].strip()\n",
    "#             for p in sample[\"answers\"][0][\"perturbation\"]\n",
    "#             if \"explanation\" in p\n",
    "#         ]\n",
    "\n",
    "#         for response_path in response_paths:\n",
    "#             with open(response_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#                 try:\n",
    "#                     model_preds = json.load(f)\n",
    "#                 except json.JSONDecodeError as e:\n",
    "#                     print(f\"❌ JSON decode error in {response_path}: {e}\")\n",
    "#                     continue\n",
    "\n",
    "#             updated = False\n",
    "#             for pred in model_preds:\n",
    "#                 if \"explanation_match\" in pred:\n",
    "#                     continue\n",
    "\n",
    "#                 model_exp = pred.get(\"explanation\", \"\").strip()\n",
    "#                 if not model_exp:\n",
    "#                     pred[\"explanation_match\"] = False\n",
    "#                     updated = True\n",
    "#                     continue\n",
    "\n",
    "#                 match_found = False\n",
    "#                 for gt_exp in gt_explanations:\n",
    "#                     prompt = f\"\"\"\n",
    "# You are evaluating whether the following model explanation captures the **same core reasoning** as the human (ground truth) explanation.\n",
    "\n",
    "# Ground Truth Explanation:\n",
    "# \"{gt_exp}\"\n",
    "\n",
    "# Model Explanation:\n",
    "# \"{model_exp}\"\n",
    "\n",
    "# Does the model explanation capture the same core reasoning as the ground truth explanation, even if phrased differently?\n",
    "\n",
    "# Answer \"yes\" or \"no\" only.\n",
    "#                     \"\"\".strip()\n",
    "\n",
    "#                     print(f\"\\n📄 Evaluating: {response_path}\")\n",
    "#                     print(f\"GT: {gt_exp}\")\n",
    "#                     print(f\"Model: {model_exp}\")\n",
    "\n",
    "#                     try:\n",
    "#                         response = evaluation_model.generate(prompt)\n",
    "#                         result_text = response.strip().lower()\n",
    "#                         print(f\"LLM response: {result_text}\")\n",
    "\n",
    "#                         if \"yes\" in result_text:\n",
    "#                             match_found = True\n",
    "#                             break\n",
    "\n",
    "#                     except ResourceExhausted as e:\n",
    "#                         print(f\"⚠️ Rate limit hit: {e}\")\n",
    "#                         print(\"⏳ Sleeping for 40 seconds...\")\n",
    "#                         time.sleep(40)\n",
    "#                         continue\n",
    "\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"⚠️ Unexpected error: {e}\")\n",
    "#                         break\n",
    "\n",
    "#                     time.sleep(1.5)\n",
    "\n",
    "#                 pred[\"explanation_match\"] = match_found\n",
    "#                 updated = True\n",
    "\n",
    "#             if updated:\n",
    "#                 with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "#                     json.dump(model_preds, f, indent=4)\n",
    "#                 print(f\"✅ Updated explanation_match in: {response_path}\")\n",
    "#             else:\n",
    "#                 print(f\"⚠️ Skipped (no update needed): {response_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Version 2 - With SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_match_sbert(dataset, responses_dir, threshold=0.8, model_name=\"all-MiniLM-L6-v2\"):\n",
    "    # Load SBERT model once\n",
    "    try:\n",
    "        sbert = SentenceTransformer(model_name)\n",
    "        sbert.to(\"cpu\")\n",
    "        print(\"✅ Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load SBERT model '{model_name}': {e}\")\n",
    "        return\n",
    "\n",
    "    for sample in tqdm.tqdm(dataset, desc=\"Evaluating explanations (SBERT)\"):\n",
    "        file_name = sample[\"file_name\"]\n",
    "\n",
    "        normalized_path = os.path.normpath(file_name)\n",
    "        subdir = os.path.dirname(normalized_path).replace(\"\\\\\", \"/\")\n",
    "        base_filename = os.path.basename(normalized_path).replace(\".json\", \"\")\n",
    "\n",
    "        pattern = os.path.join(responses_dir, \"self_consistency\", subdir, f\"{base_filename}_*.json\")\n",
    "        response_paths = sorted(glob.glob(pattern))\n",
    "\n",
    "        if not response_paths:\n",
    "            print(f\"❌ No response files found for: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        gt_explanations = [\n",
    "            p[\"explanation\"].strip()\n",
    "            for p in sample[\"answers\"][0][\"perturbation\"]\n",
    "            if \"explanation\" in p\n",
    "        ]\n",
    "\n",
    "        if not gt_explanations:\n",
    "            print(f\"⚠️ No ground truth explanations for: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        gt_embeddings = sbert.encode(gt_explanations, convert_to_tensor=True)\n",
    "\n",
    "        for response_path in response_paths:\n",
    "            try:\n",
    "                with open(response_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    model_preds = json.load(f)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"\\n❌ JSON decode error in {response_path}: {e}\")\n",
    "                \n",
    "                # Print surrounding lines for debugging\n",
    "                with open(response_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    print(\"🪵 Showing lines around error:\")\n",
    "                    for i, line in enumerate(lines):\n",
    "                        if e.lineno - 3 <= i <= e.lineno + 1:\n",
    "                            print(f\"{i + 1}: {line.strip()}\")\n",
    "                continue\n",
    "\n",
    "            updated = False\n",
    "            for pred in model_preds:\n",
    "                if \"explanation_match\" in pred:\n",
    "                    continue\n",
    "\n",
    "                model_exp = pred.get(\"explanation\", \"\").strip()\n",
    "                if not model_exp:\n",
    "                    pred[\"explanation_match\"] = False\n",
    "                    updated = True\n",
    "                    continue\n",
    "\n",
    "                model_embedding = sbert.encode(model_exp, convert_to_tensor=True)\n",
    "\n",
    "                sim_scores = util.cos_sim(model_embedding, gt_embeddings)[0]\n",
    "                max_sim = sim_scores.max().item()\n",
    "                pred[\"explanation_match_score\"] = max_sim\n",
    "                pred[\"explanation_match\"] = max_sim >= threshold\n",
    "                updated = True\n",
    "\n",
    "                print(f\"\\n📄 Evaluated: {response_path}\")\n",
    "                print(f\"GT (top sim): {gt_explanations[sim_scores.argmax().item()]}\")\n",
    "                print(f\"Model: {model_exp}\")\n",
    "                print(f\"Score: {max_sim:.4f} → {'✅ Match' if pred['explanation_match'] else '❌ No Match'}\")\n",
    "\n",
    "            if updated:\n",
    "                with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(model_preds, f, indent=4)\n",
    "                print(f\"✅ Updated explanation_match in: {response_path}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Skipped (no update needed): {response_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `evaluate_scoring`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scoring(responses_dir):\n",
    "    scores = defaultdict(lambda: {\n",
    "        \"total\": 0,\n",
    "        \"correct\": 0,\n",
    "        \"text_matches\": 0,\n",
    "        \"explanation_matches\": 0\n",
    "    })\n",
    "\n",
    "    for root, _, files in os.walk(responses_dir):\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        # Get the name of the subdirectory (e.g., \"ambiguity\")\n",
    "        subdir = os.path.basename(root)\n",
    "\n",
    "        for file in files:\n",
    "            if not file.endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            file_path = os.path.join(root, file)\n",
    "            # print(file_path)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                try:\n",
    "                    predictions = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"❌ Skipping malformed JSON: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "            for pred in predictions:\n",
    "                if not isinstance(pred, dict):\n",
    "                    continue\n",
    "                if \"text_match\" in pred and \"explanation_match\" in pred:\n",
    "                    scores[subdir][\"total\"] += 1\n",
    "                    if pred[\"text_match\"] and pred[\"explanation_match\"]:\n",
    "                        scores[subdir][\"correct\"] += 1\n",
    "                    if pred[\"text_match\"]:\n",
    "                        scores[subdir][\"text_matches\"] += 1\n",
    "                    if pred[\"explanation_match\"]:\n",
    "                        scores[subdir][\"explanation_matches\"] += 1\n",
    "\n",
    "    for subdir, stats in scores.items():\n",
    "        total = stats[\"total\"]\n",
    "        if total == 0:\n",
    "            continue\n",
    "        print(f\"\\n📁 Directory: {subdir}\")\n",
    "        print(f\"Text Match: {stats['text_matches']} / {total}\")\n",
    "        print(f\"Explanation Match: {stats['explanation_matches']} / {total}\")\n",
    "        print(f\"Text + Explanation Match: {stats['correct']} / {total}\")\n",
    "\n",
    "    return {\n",
    "        subdir: {\n",
    "            \"text_matches\": stats[\"text_matches\"],\n",
    "            \"explanation_matches\": stats[\"explanation_matches\"],\n",
    "            \"correct\": stats[\"correct\"],\n",
    "            \"total\": stats[\"total\"]\n",
    "        }\n",
    "        for subdir, stats in scores.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(\n",
    "    model: Model,\n",
    "    dataset: Dataset,\n",
    "    prompt: str,\n",
    "    responses_dir: str,\n",
    "    num_responses: int,\n",
    "    evaluation_model: Model = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the evaluation process.\n",
    "    :param model: The model to generate responses.\n",
    "    :param dataset: The dataset to evaluate.\n",
    "    :param prompt: The prompt to use for generating responses.\n",
    "    :param responses_dir: Directory to save the responses.\n",
    "    :param num_responses: The number of responses to collect per document (for self-consistency)\n",
    "    :param evaluation_model: Model for evaluating model responses.\n",
    "    \"\"\"\n",
    "    generate_responses(model, dataset, prompt, responses_dir, num_responses)\n",
    "    # explanation_match(evaluation_model, dataset, responses_dir)\n",
    "    print(\"Running explanation_match...\")\n",
    "    explanation_match_sbert(dataset, responses_dir)\n",
    "    return evaluate_scoring(responses_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUCTIONS = \"\"\"You are a legal contract expert and know how to check legal documents properly and find any discrepancies or contradictions within a file. You are also aware of all state and national laws when it comes to legal docuements.\n",
    "The file is a legal document and you are to check for any discrepancies or contradictions within the file.\n",
    "There are 10 categories when it comes to discrepancies or contradictions:\n",
    "1. Ambiguity in text - Ambiguities in text occur when key terms are **inconsistently defined within the document itself**, creating internal contradictions. This type of **in-text contradiction** confuses contract enforcement by allowing multiple interpretations of the same term in different sections, leading to potential legal disputes over meaning.\n",
    "2. Ambiguity in legal terms - Ambiguities in legal terms occur when a legal statement is vague, leading to multiple interpretations. A **legal contradiction** under this category happens when an obligation is introduced ambiguously, making it difficult to enforce under state or national law. This can result in non-compliance with regulatory requirements, leaving legal obligations open to dispute.\n",
    "3. Inconsistencies in text - Inconsistencies in text also lead to **in-text contradictions** when **different sections of a contract provide conflicting deadlines, obligations, or penalties**. This creates ambiguity regarding which terms should be enforced, leading to disputes over contractual obligations.\n",
    "4. Inconsistencies in legal terms - Inconsistencies in legal terms arise when **time-sensitive obligations** in a contract do not align with legal requirements. A **legal contradiction** in this category happens when a contract sets **a deadline or requirement that violates federal or state law**, making the contractual terms unenforceable or illegal.\n",
    "5. Misaligned in text - Misaligned terminology leads to **in-text contradictions** when the contract **uses multiple terms interchangeably without defining them**, leading to conflicting obligations.\n",
    "6. Misaligned in legal terms - Inconsistencies arise when **time-sensitive obligations** in a contract do not align with legal requirements. A **legal contradiction** in this category happens when a contract sets **a deadline or requirement that violates federal or state law**, making the contractual terms unenforceable or illegal.\n",
    "7. Omission in text - Omissions also cause **in-text contradictions** when a **key contractual clause is removed**, but **other sections still reference it**, creating an internal contradiction.\n",
    "8. Omission in legal terms - Omissions occur when a contract **removes essential information**, creating legal loopholes. A **legal contradiction** in this category happens when a contract omits **a legally mandated consumer protection**, making it non-compliant.\n",
    "9. Structural Flaws in text - this means that the text is not structured properly and does not make sense.\n",
    "10. Structural Flaws in legal terms - this means that the legal terms used in the text are not structured properly and do not make sense.\n",
    "\n",
    "Instructions:\n",
    "1. Read the file and look for the text enclosed between the tags \"<*$p$*>\" within the file.\n",
    "2. Provide a detailed explanation of why this is a discrepancy or contradiction.\n",
    "3. Provide the section where the discrepancy or contradiction exists.\n",
    "4. Provide the section location. Example: Section 5.4.                                    \n",
    "5. Categorize the discrepancy or contradiction into one of the 10 categories above (return the number of the category).\n",
    "There are 2-3 contradictions in each text.\n",
    "\n",
    "Return the results in json format. Example:\n",
    "[{\n",
    "    \"section\": \"Sponsor shall pay Club the Annual Fee for each Contract Year of this Agreement in six (6) equal installments, each\\ndue on or prior to the 1st of each month between June and November of the applicable Contract Year.\"\n",
    "    \"explanation\": \"This change introduces a contradiction regarding the payment deadline. Section 3(a) states that all installments are due by November 1st, but the added sentence allows the final payment to be made as late as December 15th without penalty. This creates ambiguity as to the actual deadline for the final installment and whether late fees would apply between November 2nd and December 15th.\"\n",
    "    \"location\": \"Section 5.2\"\n",
    "    \"category\": 3\n",
    "}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain-of-thought Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "COT = \"Make your explanations as detailed as possible and show your reasoning.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zero-shot prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = f\"\"\"{INSTRUCTIONS}\n",
    "This is the document:\n",
    "[DOCUMENT]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Few-shot prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ⚠️ **TODO: Describe the few-shot**\n",
    "- Feed 1 entire document with tokens to the LLM for the few-shot. Describe that what's enclosed in the token is perturbed text.\n",
    "- Show the correct un-perturbed text. Not the doc.\n",
    "- Keep in mind each perturbed doc has 3 tokened parts.\n",
    "- Keep it category specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt design:\n",
    "- Keep the `INSTRUCTION` variable\n",
    "- Feed the whole [Perturbed Document] Tell the LLM the text enclosed in the <*$p$*><*$p$*> tokens are the perturbed parts\n",
    "- [The the line Original Unperturbed Document] - Take it from the .json\n",
    "- Then feed it the test document `[DOCUMENT]` and tell it to answer\n",
    "- Keep it category specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = INSTRUCTIONS + \"\"\"\n",
    "\n",
    "Question:\n",
    "Section 1.1 Ambiguities - In Text Contradiction: Adaptimmune shall have responsibility for IND filing and monitoring unless otherwise agreed by JSC.\n",
    "Section 2.9 Ambiguities - In Text Contradiction: MD Anderson and Adaptimmune will promptly notify each other upon identifying any aspect of a Protocol, including information discovered during site monitoring visits, or Study results that may adversely affect the safety, well-being, or medical care of the Study subjects, or that may affect the willingness of Study subjects to continue participation in a Study, influence the conduct of the Study, or that may alter the IRB's approval to continue the Study.\n",
    "Section 8.3 Ambiguities - In Text Contradiction: The Parties agree that any termination of a Study Order shall allow for: (i) the wind down of the Study to ensure the safety of Study subjects; and (ii) Adaptimmune's final reconciliation of Data related to the Study in addition to Adaptimmune's final monitoring visit.\n",
    "\n",
    "Answer:\n",
    "[\n",
    "  {\n",
    "    \"section\": \"Adaptimmune shall have responsibility for IND filing and monitoring unless otherwise agreed by JSC.\",\n",
    "    \"explanation\": \"This change introduces a contradiction regarding the responsibility for IND monitoring. The original text assigns it to Adaptimmune unless the JSC decides otherwise. The modified version definitively assigns monitoring to MD Anderson, creating a conflict if the JSC makes a different decision later, or if other sections assume Adaptimmune's monitoring role.\",\n",
    "    \"location\": \"1.1\",\n",
    "    \"category\": 1\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"MD Anderson and Adaptimmune will promptly notify each other upon identifying any aspect of a Protocol, including information discovered during site monitoring visits, or Study results that may adversely affect the safety, well-being, or medical care of the Study subjects, or that may affect the willingness of Study subjects to continue participation in a Study, influence the conduct of the Study, or that may alter the IRB's approval to continue the Study.\",\n",
    "    \"explanation\": \"This edit creates conflicting requirements for reporting adverse findings. Previously, both parties were responsible for mutual notification. Now, Adaptimmune's notification to MD Anderson is limited to data results from *Adaptimmune's* monitoring. If MD Anderson discovers issues through their own oversight, it's unclear if Adaptimmune should be notified, creating uncertainty in communication and potential safety oversight.\",\n",
    "    \"location\": \"2.9\",\n",
    "    \"category\": 1\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"The Parties agree that any termination of a Study Order shall allow for: (i) the wind down of the Study to ensure the safety of Study subjects; and (ii) Adaptimmune's final reconciliation of Data related to the Study in addition to Adaptimmune's final monitoring visit.\",\n",
    "    \"explanation\": \"Conflicting responsibilities are defined, at termination. Data reconcilation responsibilities is given to both parites.\",\n",
    "    \"location\": \"8.3\",\n",
    "    \"category\": 1\n",
    "  }\n",
    "]\n",
    "\n",
    "Question:\n",
    "Section 5.1 Ambiguities - In Text Contradiction: The term of this Agreement shall commence on the Effective Date and continue for six (6) months.\n",
    "Section 3.2 Ambiguities - In Text Contradiction: Detto shall pay to PivX for each unit of Qwik-Fix Pro distributed hereunder per copy fees (the \"Per Copy Fees\") as governed by the terms set forth in Exhibit A. In the event that PivX changes the Third Party prices, Per Copy Fees based on such changed prices shall apply to any order for Qwik-Fix Pro received by PivX after the effective date of the increase. PivX shall provide Detto with at least forty-five (45) days written notice of any increase in the Per Copy Fees.\n",
    "Section 1.3 Ambiguities - In Text Contradiction: PivX grants Detto access to PivX ChannelNet as governed by the terms set forth in Exhibit A.\n",
    "\n",
    "Answer:\n",
    "[\n",
    "  {\n",
    "    \"section\": \"The term of this Agreement shall commence on the Effective Date and continue for six (6) months.\",\n",
    "    \"explanation\": \"The original text specifies a fixed six-month term. The modified text introduces an automatic renewal clause, conflicting with the initial fixed term. This creates uncertainty about the agreement's duration and renewal conditions.\",\n",
    "    \"location\": \"5.1\",\n",
    "    \"category\": 3\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"Detto shall pay to PivX for each unit of Qwik-Fix Pro distributed hereunder per copy fees (the \\\"Per Copy Fees\\\") as governed by the terms set forth in Exhibit A. In the event that PivX changes the Third Party prices, Per Copy Fees based on such changed prices shall apply to any order for Qwik-Fix Pro received by PivX after the effective date of the increase. PivX shall provide Detto with at least forty-five (45) days written notice of any increase in the Per Copy Fees.\",\n",
    "    \"explanation\": \"The original clause stipulates a 45-day notice period for Per Copy Fee increases. The modified text allows immediate price changes, creating a direct contradiction. This introduces uncertainty for Detto regarding pricing stability and predictability, potentially leading to disputes.\",\n",
    "    \"location\": \"3.2\",\n",
    "    \"category\": 3\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"PivX grants Detto access to PivX ChannelNet as governed by the terms set forth in Exhibit A.\",\n",
    "    \"explanation\": \"Originally, access to PivX ChannelNet is presented as governed by Exhibit A, implying some defined terms and conditions. The change introduces an arbitrary right for PivX to terminate access at any time, contradicting the idea of governance by Exhibit A and creating uncertainty regarding the reliability and scope of Detto's access.\",\n",
    "    \"location\": \"1.3\",\n",
    "    \"category\": 3\n",
    "  }\n",
    "]\n",
    "\n",
    "Question:\n",
    "Section 1.3 Misaligned Terminology - In Text Contradiction: Adaptimmune agrees to commit funding in an amount of at least nineteen million six hundred and forty four thousand Dollars US ($19,644,000) for the performance of the Studies as set out in Exhibit I during the term (\"Alliance Funding\"). The JSC may allocate and/or re-allocate funds to Studies as necessary and agreed by JSC.\n",
    "Section 8.3 Misaligned Terminology - In Text Contradiction: The Parties agree that any termination of a Study Order shall allow for: (i) the wind down of the Study to ensure the safety of Study subjects; and (ii) Adaptimmune's final reconciliation of Data related to the Study in addition to Adaptimmune's final monitoring visit. All reasonable fees associated with the wind-down activities and final monitoring visit shall be paid by Adaptimmune, to the extent not covered by Alliance Funding.\n",
    "Section 10.1 Misaligned Terminology - In Text Contradiction: Adaptimmune shall assume responsibility for reasonable medical expenses incurred by a Study subject for reasonable and necessary treatment if the Study subject experiences an illness, adverse event or injury that is a result of the Study Drug or any procedure required by the Protocol that the subject would not have undergone were it not for such Study subject's participation in the Study. Adaptimmune shall not be responsible for expenses to the extent that they are due to pre-existing medical conditions, underlying disease, or the negligence or intentional misconduct or due to breach of this Agreement by MD Anderson or Principal Investigator.\n",
    "\n",
    "Answer:\n",
    "[\n",
    "  {\n",
    "    \"section\": \"Adaptimmune agrees to commit funding in an amount of at least nineteen million six hundred and forty four thousand Dollars US ($19,644,000) for the performance of the Studies as set out in Exhibit I during the term (\\\"Alliance Funding\\\"). The JSC may allocate and/or re-allocate funds to Studies as necessary and agreed by JSC.\",\n",
    "    \"explanation\": \"This introduces ambiguity and contradiction by changing 'allocate and/or re-allocate funds to Studies as necessary and agreed by JSC' to 'adjust funds to Studies as necessary', granting the JSC unilateral control without requiring agreement from both parties. This creates uncertainty about whether Adaptimmune has a say in how the funds are spent.\",\n",
    "    \"location\": \"1.3\",\n",
    "    \"category\": 5\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"The Parties agree that any termination of a Study Order shall allow for: (i) the wind down of the Study to ensure the safety of Study subjects; and (ii) Adaptimmune's final reconciliation of Data related to the Study in addition to Adaptimmune's final monitoring visit. All reasonable fees associated with the wind-down activities and final monitoring visit shall be paid by Adaptimmune, to the extent not covered by Alliance Funding.\",\n",
    "    \"explanation\": \"This change creates a contradiction regarding who is responsible for the fees associated with winding down a study. The original text states that Adaptimmune pays 'all reasonable fees...to the extent not covered by Alliance Funding', whereas the modified text states that all fees are split equally by Adaptimmune and MD Anderson 'regardless of Alliance Funding.' This uncertainty could lead to disputes.\",\n",
    "    \"location\": \"8.3\",\n",
    "    \"category\": 5\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"Adaptimmune shall assume responsibility for reasonable medical expenses incurred by a Study subject for reasonable and necessary treatment if the Study subject experiences an illness, adverse event or injury that is a result of the Study Drug or any procedure required by the Protocol that the subject would not have undergone were it not for such Study subject's participation in the Study. Adaptimmune shall not be responsible for expenses to the extent that they are due to pre-existing medical conditions, underlying disease, or the negligence or intentional misconduct or due to breach of this Agreement by MD Anderson or Principal Investigator.\",\n",
    "    \"explanation\": \"This modifies 'shall assume responsibility' to 'may, at its sole discretion, cover', which introduces ambiguity regarding Adaptimmune's obligation to cover medical expenses. In original text, it's a requirement but in the changed one, it is up to Adaptimmune's decision. This contradiction creates uncertainty about patient care costs.\",\n",
    "    \"location\": \"10.1\",\n",
    "    \"category\": 5\n",
    "  }\n",
    "]\n",
    "\n",
    "Question:\n",
    "Section 7.2 Omissions - In Text Contradiction: Patient records, research notebooks, all original source documents, Protected Health Information (as such term is defined by HIPAA), MD Anderson's business records, regulatory and compliance documents, original medical records or any information required to be maintained by MD Anderson in accordance with Applicable Laws, that is generated in the conduct of the Studies (collectively, \"MD Anderson Records\") will be owned by MD Anderson. All results, data and work product (excluding MD Anderson Records) generated in the conduct of the Studies (\"Data\") shall be owned by Adaptimmune Limited. MD Anderson shall maintain all such Data as confidential, subject to the publication rights granted in Section 12 below. Data will be promptly disclosed by MD Anderson to Adaptimmune in the form of a Study report or as otherwise reasonably requested by Adaptimmune. Notwithstanding any other provision of this Agreement, MD Anderson shall have the right to use results and Data of the Study for its internal research, academic, and patient care purposes and for publication in accordance with Section 12 below, save that no right or license is granted to MD Anderson under any of Adaptimmune's Background IP. Adaptimmune shall promptly disclose any Data it generates to MD Anderson.\n",
    "Section 8.3 Omissions - In Text Contradiction: A Party may terminate a Study Order: (a) if the other Party commits a material breach of this Agreement or the Study Order and fails to cure such breach within thirty (30) days of receiving notice from the non-breaching Party of such breach; or (b) in the case of any Clinical Studies, due to health and safety concerns related to the Study Drug or procedures in the Study (including regulatory holds due to the health and safety of the Study Subjects); or (c) in the case of MD Anderson and in relation to any Clinical Studies, where IRB requests termination of any Study; or (d) in the case of Adaptimmune, *** set out in Section 1.2 above. The Parties agree that any termination of a Study Order shall allow for: (i) the wind down of the Study to ensure the safety of Study subjects; and (ii) Adaptimmune's final reconciliation of Data related to the Study in addition to Adaptimmune's final monitoring visit. All reasonable fees associated with the wind-down activities and final monitoring visit shall be paid by Adaptimmune, to the extent not covered by Alliance Funding. Termination of one or more Study Orders will not automatically result in the termination of this Agreement or termination of any other Study Orders. Upon termination of a Study Order, MD Anderson will immediately return (at Adaptimmune's cost) any Study Drugs provided by Adaptimmune for such Study as directed by Adaptimmune.\n",
    "Section 12.2 Omissions - In Text Contradiction: Clinical Studies: In relation to any Clinical Study, Adaptimmune shall have the *** right to publish or publicly disclose any Data or results arising from such Clinical Study including where such publication arises from the submission of data and/or results to the regulatory authorities. Such right to publish shall not include any MD Anderson Records or any public health information protected by HIPAA or where any publication would be in breach of the Consent and/or Authorization. MD Anderson and/or Principal Investigator shall have the right to independently publish or publicly disclose, either in writing or orally, the Data and results of the Clinical Study/ies after the earlier of the (i) first publication (including any multi-site publication) of such Data and/or results; (ii) twelve (12) months after completion of any multi-site study encompassing any Study or if none, six (6) months after completion of Study. MD Anderson shall, at least thirty (30) days ahead of any proposed date for submission, furnish Adaptimmune with a written copy of the proposed publication or public disclosure. Within such thirty (30) day period, Adaptimmune shall review such proposed publication for any Confidential Information of Adaptimmune provided hereunder or patentable Data. Adaptimmune may also comment on such proposed publication and MD Anderson shall consider such comments in good faith during the aforementioned thirty (30) day period. MD Anderson and/or Principal Investigator shall remove Confidential Information of Adaptimmune provided hereunder that has been so identified (other than Data or Study results), provided that Adaptimmune agrees to act in good faith when requiring the deletion of Adaptimmune Confidential Information. In addition Adaptimmune may request delay of publication for a period not to exceed *** (***) days from the date of receipt of request by MD Anderson, to permit Adaptimmune or Adaptimmune Limited or any Joint Research Partner to file patent applications or to otherwise seek to protect any intellectual property rights contained in such publication or disclosure. Upon such request, MD Anderson shall delay such publication until the relevant protection is filed up to a maximum of *** (***) days from date of receipt of request for delay by MD Anderson.\n",
    "\n",
    "Answer:\n",
    "[\n",
    "  {\n",
    "    \"section\": \"Patient records, research notebooks, all original source documents, Protected Health Information (as such term is defined by HIPAA), MD Anderson's business records, regulatory and compliance documents, original medical records or any information required to be maintained by MD Anderson in accordance with Applicable Laws, that is generated in the conduct of the Studies (collectively, \\\"MD Anderson Records\\\") will be owned by MD Anderson. All results, data and work product (excluding MD Anderson Records) generated in the conduct of the Studies (\\\"Data\\\") shall be owned by Adaptimmune Limited. MD Anderson shall maintain all such Data as confidential, subject to the publication rights granted in Section 12 below. Data will be promptly disclosed by MD Anderson to Adaptimmune in the form of a Study report or as otherwise reasonably requested by Adaptimmune. Notwithstanding any other provision of this Agreement, MD Anderson shall have the right to use results and Data of the Study for its internal research, academic, and patient care purposes and for publication in accordance with Section 12 below, save that no right or license is granted to MD Anderson under any of Adaptimmune's Background IP. Adaptimmune shall promptly disclose any Data it generates to MD Anderson.\",\n",
    "    \"explanation\": \"By removing the section 'Protected Health Information (as such term is defined by HIPAA)', this creates an uncertainty on how to treat this information as it could now be considered 'Data', which is owned by Adaptimmune Limited. This would make the treatment of PHI ambiguous and could lead to breaches of HIPAA if Adaptimmune treats it as regular data.\",\n",
    "    \"location\": \"7.2\",\n",
    "    \"category\": 7\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"A Party may terminate a Study Order: (a) if the other Party commits a material breach of this Agreement or the Study Order and fails to cure such breach within thirty (30) days of receiving notice from the non-breaching Party of such breach; or (b) in the case of any Clinical Studies, due to health and safety concerns related to the Study Drug or procedures in the Study (including regulatory holds due to the health and safety of the Study Subjects); or (c) in the case of MD Anderson and in relation to any Clinical Studies, where IRB requests termination of any Study; or (d) in the case of Adaptimmune, *** set out in Section 1.2 above. The Parties agree that any termination of a Study Order shall allow for: (i) the wind down of the Study to ensure the safety of Study subjects; and (ii) Adaptimmune's final reconciliation of Data related to the Study in addition to Adaptimmune's final monitoring visit. All reasonable fees associated with the wind-down activities and final monitoring visit shall be paid by Adaptimmune, to the extent not covered by Alliance Funding. Termination of one or more Study Orders will not automatically result in the termination of this Agreement or termination of any other Study Orders. Upon termination of a Study Order, MD Anderson will immediately return (at Adaptimmune's cost) any Study Drugs provided by Adaptimmune for such Study as directed by Adaptimmune.\",\n",
    "    \"explanation\": \"The section '(d) in the case of Adaptimmune, *** set out in Section 1.2 above' contains an omission already in the original contract. By removing the asterisks, it suggests that Adaptimmune does have a unilateral right to terminate, but the grounds are unknown because the text is still omitted as the text references Section 1.2 above. Thus Section 1.2 is an essential condition for Adaptimmune to terminate, but the details are omitted.\",\n",
    "    \"location\": \"8.3\",\n",
    "    \"category\": 7\n",
    "  },\n",
    "  {\n",
    "    \"section\": \"Clinical Studies: In relation to any Clinical Study, Adaptimmune shall have the *** right to publish or publicly disclose any Data or results arising from such Clinical Study including where such publication arises from the submission of data and/or results to the regulatory authorities. Such right to publish shall not include any MD Anderson Records or any public health information protected by HIPAA or where any publication would be in breach of the Consent and/or Authorization. MD Anderson and/or Principal Investigator shall have the right to independently publish or publicly disclose, either in writing or orally, the Data and results of the Clinical Study/ies after the earlier of the (i) first publication (including any multi-site publication) of such Data and/or results; (ii) twelve (12) months after completion of any multi-site study encompassing any Study or if none, six (6) months after completion of Study. MD Anderson shall, at least thirty (30) days ahead of any proposed date for submission, furnish Adaptimmune with a written copy of the proposed publication or public disclosure. Within such thirty (30) day period, Adaptimmune shall review such proposed publication for any Confidential Information of Adaptimmune provided hereunder or patentable Data. Adaptimmune may also comment on such proposed publication and MD Anderson shall consider such comments in good faith during the aforementioned thirty (30) day period. MD Anderson and/or Principal Investigator shall remove Confidential Information of Adaptimmune provided hereunder that has been so identified (other than Data or Study results), provided that Adaptimmune agrees to act in good faith when requiring the deletion of Adaptimmune Confidential Information. In addition Adaptimmune may request delay of publication for a period not to exceed *** (***) days from the date of receipt of request by MD Anderson, to permit Adaptimmune or Adaptimmune Limited or any Joint Research Partner to file patent applications or to otherwise seek to protect any intellectual property rights contained in such publication or disclosure. Upon such request, MD Anderson shall delay such publication until the relevant protection is filed up to a maximum of *** (***) days from date of receipt of request for delay by MD Anderson.\",\n",
    "    \"explanation\": \"In the original text, there are asterisks indicating that some wording is omitted, which means that there is a term that is not defined. In the changed text, the asterisks have been removed from 'Adaptimmune shall have the *** right to publish'. By removing this asterisks, it implies that Adaptimmune can publish or publicly disclose any data, giving them full rights. However, the type or extent of these rights is not defined, creating uncertainty.\",\n",
    "    \"location\": \"12.2\",\n",
    "    \"category\": 7\n",
    "  }\n",
    "]\n",
    "\n",
    "Question: [DOCUMENT]\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero-shot variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects path name such that it ignores path length limit and formats based on your OS definition\n",
    "def correct_path_name(path):\n",
    "    return r\"\\\\?\\{}\".format(os.path.abspath(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples:   0%|          | 0/25 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Processing samples:  16%|█▌        | 4/25 [00:00<00:00, 38.98it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing samples:  32%|███▏      | 8/25 [00:00<00:00, 30.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing samples:  48%|████▊     | 12/25 [00:00<00:00, 27.43it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing samples:  64%|██████▍   | 16/25 [00:00<00:00, 29.42it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing samples:  80%|████████  | 20/25 [00:00<00:00, 28.70it/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing samples: 100%|██████████| 25/25 [00:00<00:00, 27.76it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Processing samples: 100%|██████████| 25/25 [00:00<00:00, 25.15it/s]\n",
      "Processing samples: 100%|██████████| 25/25 [00:00<00:00, 26.43it/s]\n",
      "Processing samples: 100%|██████████| 25/25 [00:00<00:00, 28.10it/s]\n",
      "Processing samples: 100%|██████████| 25/25 [00:00<00:00, 25.44it/s]\n",
      "Processing samples: 100%|██████████| 25/25 [00:01<00:00, 24.43it/s]\n",
      "Processing samples: 100%|██████████| 25/25 [00:00<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running explanation_match...\n",
      "Running explanation_match...\n",
      "Running explanation_match...\n",
      "Running explanation_match...\n",
      "Running explanation_match...\n",
      "Running explanation_match...\n",
      "Running explanation_match...\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 2 / 15\n",
      "Explanation Match: 1 / 15\n",
      "Text + Explanation Match: 0 / 15\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 5 / 15\n",
      "Explanation Match: 3 / 15\n",
      "Text + Explanation Match: 2 / 15\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 2 / 15\n",
      "Explanation Match: 3 / 15\n",
      "Text + Explanation Match: 2 / 15\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 2 / 15\n",
      "Explanation Match: 1 / 15\n",
      "Text + Explanation Match: 1 / 15\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 11 / 15\n",
      "Explanation Match: 7 / 15\n",
      "Text + Explanation Match: 6 / 15\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 7 / 14\n",
      "Explanation Match: 9 / 14\n",
      "Text + Explanation Match: 4 / 14\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 7 / 15\n",
      "Explanation Match: 8 / 15\n",
      "Text + Explanation Match: 4 / 15\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 11 / 15\n",
      "Explanation Match: 6 / 15\n",
      "Text + Explanation Match: 5 / 15\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 6 / 17\n",
      "Explanation Match: 5 / 17\n",
      "Text + Explanation Match: 3 / 17\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 11 / 16\n",
      "Explanation Match: 14 / 16\n",
      "Text + Explanation Match: 10 / 16\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 7 / 15\n",
      "Explanation Match: 11 / 15\n",
      "Text + Explanation Match: 7 / 15\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 8 / 16\n",
      "Explanation Match: 8 / 16\n",
      "Text + Explanation Match: 5 / 16\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 12 / 15\n",
      "Explanation Match: 8 / 15\n",
      "Text + Explanation Match: 7 / 15\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 6 / 17\n",
      "Explanation Match: 4 / 17\n",
      "Text + Explanation Match: 2 / 17\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 10 / 17\n",
      "Explanation Match: 6 / 17\n",
      "Text + Explanation Match: 4 / 17\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 7 / 15\n",
      "Explanation Match: 9 / 15\n",
      "Text + Explanation Match: 5 / 15\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 9 / 15\n",
      "Explanation Match: 8 / 15\n",
      "Text + Explanation Match: 5 / 15\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 11 / 15\n",
      "Explanation Match: 4 / 15\n",
      "Text + Explanation Match: 3 / 15\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 6 / 15\n",
      "Explanation Match: 5 / 15\n",
      "Text + Explanation Match: 2 / 15\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 13 / 15\n",
      "Explanation Match: 6 / 15\n",
      "Text + Explanation Match: 6 / 15\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 11 / 76\n",
      "Explanation Match: 23 / 76\n",
      "Text + Explanation Match: 11 / 76\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 27 / 75\n",
      "Explanation Match: 19 / 75\n",
      "Text + Explanation Match: 17 / 75\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 11 / 75\n",
      "Explanation Match: 16 / 75\n",
      "Text + Explanation Match: 10 / 75\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 8 / 75\n",
      "Explanation Match: 9 / 75\n",
      "Text + Explanation Match: 8 / 75\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 8 / 15\n",
      "Explanation Match: 8 / 15\n",
      "Text + Explanation Match: 4 / 15\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 7 / 15\n",
      "Explanation Match: 12 / 15\n",
      "Text + Explanation Match: 7 / 15\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 10 / 15\n",
      "Explanation Match: 4 / 15\n",
      "Text + Explanation Match: 3 / 15\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 12 / 15\n",
      "Explanation Match: 6 / 15\n",
      "Text + Explanation Match: 5 / 15\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 6 / 17\n",
      "Explanation Match: 3 / 17\n",
      "Text + Explanation Match: 2 / 17\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 9 / 16\n",
      "Explanation Match: 10 / 16\n",
      "Text + Explanation Match: 5 / 16\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 2 / 15\n",
      "Explanation Match: 1 / 15\n",
      "Text + Explanation Match: 0 / 15\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 5 / 15\n",
      "Explanation Match: 4 / 15\n",
      "Text + Explanation Match: 3 / 15\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 4 / 15\n",
      "Explanation Match: 1 / 15\n",
      "Text + Explanation Match: 1 / 15\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 2 / 15\n",
      "Explanation Match: 2 / 15\n",
      "Text + Explanation Match: 2 / 15\n",
      "\n",
      "📁 Directory: structural_flaws_inText\n",
      "Text Match: 11 / 15\n",
      "Explanation Match: 6 / 15\n",
      "Text + Explanation Match: 4 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 25/25 [00:28<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running explanation_match...\n",
      "❌ Failed to load SBERT model 'all-MiniLM-L6-v2': Cannot copy out of meta tensor; no data! Please use torch.nn.Module.to_empty() instead of torch.nn.Module.to() when moving module from meta to a different device.\n",
      "\n",
      "📁 Directory: ambiguity_inText\n",
      "Text Match: 9 / 21\n",
      "Explanation Match: 5 / 21\n",
      "Text + Explanation Match: 1 / 21\n",
      "\n",
      "📁 Directory: inconsistencies_inText\n",
      "Text Match: 12 / 19\n",
      "Explanation Match: 6 / 19\n",
      "Text + Explanation Match: 4 / 19\n",
      "\n",
      "📁 Directory: misaligned_terminalogy_inText\n",
      "Text Match: 15 / 19\n",
      "Explanation Match: 3 / 19\n",
      "Text + Explanation Match: 3 / 19\n",
      "\n",
      "📁 Directory: omissions_inText\n",
      "Text Match: 7 / 20\n",
      "Explanation Match: 3 / 20\n",
      "Text + Explanation Match: 2 / 20\n",
      "✅ DONE\n"
     ]
    }
   ],
   "source": [
    "runs = [\n",
    "    {\n",
    "        \"name\": \"zero-shot\",\n",
    "        \"model\": GeminiModel(API_KEYS),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": zero_shot_prompt,\n",
    "        \"responses_dir\": correct_path_name(\"mini-eval/responses/zero-shot/\"),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"zero-shot-cot\",\n",
    "        \"model\": GeminiModel(API_KEYS),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": zero_shot_prompt + COT,\n",
    "        \"responses_dir\": correct_path_name(\"mini-eval/responses/zero-shot-cot/\"),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"zero-shot-self-verification\",\n",
    "        \"model\": SelfVerificationModel(GeminiModel(API_KEYS)),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": zero_shot_prompt,\n",
    "        \"responses_dir\": correct_path_name(\n",
    "            \"mini-eval/responses/zero-shot-self-verification/\"\n",
    "        ),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"zero-shot-self-verification-cot\",\n",
    "        \"model\": SelfVerificationModel(GeminiModel(API_KEYS)),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": zero_shot_prompt + COT,\n",
    "        \"responses_dir\": correct_path_name(\n",
    "            \"mini-eval/responses/zero-shot-self-verification-cot/\"\n",
    "        ),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"few-shot\",\n",
    "        \"model\": GeminiModel(API_KEYS),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": few_shot_prompt,\n",
    "        \"responses_dir\": correct_path_name(\"mini-eval/responses/few-shot/\"),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"few-shot-cot\",\n",
    "        \"model\": GeminiModel(API_KEYS),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": few_shot_prompt + COT,\n",
    "        \"responses_dir\": correct_path_name(\"mini-eval/responses/few-shot-cot/\"),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"few-shot-self-verification\",\n",
    "        \"model\": SelfVerificationModel(GeminiModel(API_KEYS)),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": few_shot_prompt,\n",
    "        \"responses_dir\": correct_path_name(\n",
    "            \"mini-eval/responses/few-shot-self-verification/\"\n",
    "        ),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"few-shot-self-verification-cot\",\n",
    "        \"model\": SelfVerificationModel(GeminiModel(API_KEYS)),\n",
    "        \"dataset\": MiniEvalDataset(),\n",
    "        \"prompt\": few_shot_prompt + COT,\n",
    "        \"responses_dir\": correct_path_name(\n",
    "            \"mini-eval/responses/few-shot-self-verification-cot/\"\n",
    "        ),\n",
    "        \"num_responses\": 1,\n",
    "        \"evaluation_model\": GeminiModel(API_KEYS),\n",
    "    },\n",
    "]\n",
    "\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Silence stdout and stderr\n",
    "import sys\n",
    "import os\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    with open(os.devnull, \"w\") as fnull:\n",
    "        with contextlib.redirect_stdout(fnull), contextlib.redirect_stderr(fnull):\n",
    "            yield\n",
    "\n",
    "\n",
    "# Semaphore to limit the number of concurrent threads to the number of API keys\n",
    "api_key_semaphore = threading.Semaphore(len(API_KEYS))\n",
    "\n",
    "run_results = {}\n",
    "\n",
    "\n",
    "def run_with_semaphore(run_config):\n",
    "    \"\"\"\n",
    "    Wrapper function to run a task while respecting the semaphore.\n",
    "    \"\"\"\n",
    "    with api_key_semaphore:\n",
    "        run_results[run_config[\"name\"]] = run(\n",
    "                model=run_config[\"model\"],\n",
    "                dataset=run_config[\"dataset\"],\n",
    "                prompt=run_config[\"prompt\"],\n",
    "                responses_dir=run_config[\"responses_dir\"],\n",
    "                num_responses=run_config[\"num_responses\"],\n",
    "                evaluation_model=run_config[\"evaluation_model\"],\n",
    "            )\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=len(API_KEYS)) as executor:\n",
    "    for run_config in runs:\n",
    "        executor.submit(run_with_semaphore, run_config)\n",
    "\n",
    "print(\"✅ DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguity_inText</th>\n",
       "      <th>inconsistencies_inText</th>\n",
       "      <th>misaligned_terminalogy_inText</th>\n",
       "      <th>omissions_inText</th>\n",
       "      <th>structural_flaws_inText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero-shot-self-verification-cot</th>\n",
       "      <td>{'text_matches': 2, 'explanation_matches': 1, ...</td>\n",
       "      <td>{'text_matches': 5, 'explanation_matches': 3, ...</td>\n",
       "      <td>{'text_matches': 2, 'explanation_matches': 3, ...</td>\n",
       "      <td>{'text_matches': 2, 'explanation_matches': 1, ...</td>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot-self-verification-cot</th>\n",
       "      <td>{'text_matches': 7, 'explanation_matches': 9, ...</td>\n",
       "      <td>{'text_matches': 7, 'explanation_matches': 8, ...</td>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 6,...</td>\n",
       "      <td>{'text_matches': 6, 'explanation_matches': 5, ...</td>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot-cot</th>\n",
       "      <td>{'text_matches': 7, 'explanation_matches': 11,...</td>\n",
       "      <td>{'text_matches': 8, 'explanation_matches': 8, ...</td>\n",
       "      <td>{'text_matches': 12, 'explanation_matches': 8,...</td>\n",
       "      <td>{'text_matches': 6, 'explanation_matches': 4, ...</td>\n",
       "      <td>{'text_matches': 10, 'explanation_matches': 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>{'text_matches': 7, 'explanation_matches': 9, ...</td>\n",
       "      <td>{'text_matches': 9, 'explanation_matches': 8, ...</td>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 4,...</td>\n",
       "      <td>{'text_matches': 6, 'explanation_matches': 5, ...</td>\n",
       "      <td>{'text_matches': 13, 'explanation_matches': 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot-cot</th>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 23...</td>\n",
       "      <td>{'text_matches': 27, 'explanation_matches': 19...</td>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 16...</td>\n",
       "      <td>{'text_matches': 8, 'explanation_matches': 9, ...</td>\n",
       "      <td>{'text_matches': 8, 'explanation_matches': 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot-self-verification</th>\n",
       "      <td>{'text_matches': 7, 'explanation_matches': 12,...</td>\n",
       "      <td>{'text_matches': 10, 'explanation_matches': 4,...</td>\n",
       "      <td>{'text_matches': 12, 'explanation_matches': 6,...</td>\n",
       "      <td>{'text_matches': 6, 'explanation_matches': 3, ...</td>\n",
       "      <td>{'text_matches': 9, 'explanation_matches': 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot-self-verification</th>\n",
       "      <td>{'text_matches': 2, 'explanation_matches': 1, ...</td>\n",
       "      <td>{'text_matches': 5, 'explanation_matches': 4, ...</td>\n",
       "      <td>{'text_matches': 4, 'explanation_matches': 1, ...</td>\n",
       "      <td>{'text_matches': 2, 'explanation_matches': 2, ...</td>\n",
       "      <td>{'text_matches': 11, 'explanation_matches': 6,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>{'text_matches': 9, 'explanation_matches': 5, ...</td>\n",
       "      <td>{'text_matches': 12, 'explanation_matches': 6,...</td>\n",
       "      <td>{'text_matches': 15, 'explanation_matches': 3,...</td>\n",
       "      <td>{'text_matches': 7, 'explanation_matches': 3, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  ambiguity_inText  \\\n",
       "zero-shot-self-verification-cot  {'text_matches': 2, 'explanation_matches': 1, ...   \n",
       "few-shot-self-verification-cot   {'text_matches': 7, 'explanation_matches': 9, ...   \n",
       "few-shot-cot                     {'text_matches': 7, 'explanation_matches': 11,...   \n",
       "few-shot                         {'text_matches': 7, 'explanation_matches': 9, ...   \n",
       "zero-shot-cot                    {'text_matches': 11, 'explanation_matches': 23...   \n",
       "few-shot-self-verification       {'text_matches': 7, 'explanation_matches': 12,...   \n",
       "zero-shot-self-verification      {'text_matches': 2, 'explanation_matches': 1, ...   \n",
       "zero-shot                        {'text_matches': 9, 'explanation_matches': 5, ...   \n",
       "\n",
       "                                                            inconsistencies_inText  \\\n",
       "zero-shot-self-verification-cot  {'text_matches': 5, 'explanation_matches': 3, ...   \n",
       "few-shot-self-verification-cot   {'text_matches': 7, 'explanation_matches': 8, ...   \n",
       "few-shot-cot                     {'text_matches': 8, 'explanation_matches': 8, ...   \n",
       "few-shot                         {'text_matches': 9, 'explanation_matches': 8, ...   \n",
       "zero-shot-cot                    {'text_matches': 27, 'explanation_matches': 19...   \n",
       "few-shot-self-verification       {'text_matches': 10, 'explanation_matches': 4,...   \n",
       "zero-shot-self-verification      {'text_matches': 5, 'explanation_matches': 4, ...   \n",
       "zero-shot                        {'text_matches': 12, 'explanation_matches': 6,...   \n",
       "\n",
       "                                                     misaligned_terminalogy_inText  \\\n",
       "zero-shot-self-verification-cot  {'text_matches': 2, 'explanation_matches': 3, ...   \n",
       "few-shot-self-verification-cot   {'text_matches': 11, 'explanation_matches': 6,...   \n",
       "few-shot-cot                     {'text_matches': 12, 'explanation_matches': 8,...   \n",
       "few-shot                         {'text_matches': 11, 'explanation_matches': 4,...   \n",
       "zero-shot-cot                    {'text_matches': 11, 'explanation_matches': 16...   \n",
       "few-shot-self-verification       {'text_matches': 12, 'explanation_matches': 6,...   \n",
       "zero-shot-self-verification      {'text_matches': 4, 'explanation_matches': 1, ...   \n",
       "zero-shot                        {'text_matches': 15, 'explanation_matches': 3,...   \n",
       "\n",
       "                                                                  omissions_inText  \\\n",
       "zero-shot-self-verification-cot  {'text_matches': 2, 'explanation_matches': 1, ...   \n",
       "few-shot-self-verification-cot   {'text_matches': 6, 'explanation_matches': 5, ...   \n",
       "few-shot-cot                     {'text_matches': 6, 'explanation_matches': 4, ...   \n",
       "few-shot                         {'text_matches': 6, 'explanation_matches': 5, ...   \n",
       "zero-shot-cot                    {'text_matches': 8, 'explanation_matches': 9, ...   \n",
       "few-shot-self-verification       {'text_matches': 6, 'explanation_matches': 3, ...   \n",
       "zero-shot-self-verification      {'text_matches': 2, 'explanation_matches': 2, ...   \n",
       "zero-shot                        {'text_matches': 7, 'explanation_matches': 3, ...   \n",
       "\n",
       "                                                           structural_flaws_inText  \n",
       "zero-shot-self-verification-cot  {'text_matches': 11, 'explanation_matches': 7,...  \n",
       "few-shot-self-verification-cot   {'text_matches': 11, 'explanation_matches': 14...  \n",
       "few-shot-cot                     {'text_matches': 10, 'explanation_matches': 6,...  \n",
       "few-shot                         {'text_matches': 13, 'explanation_matches': 6,...  \n",
       "zero-shot-cot                    {'text_matches': 8, 'explanation_matches': 8, ...  \n",
       "few-shot-self-verification       {'text_matches': 9, 'explanation_matches': 10,...  \n",
       "zero-shot-self-verification      {'text_matches': 11, 'explanation_matches': 6,...  \n",
       "zero-shot                                                                      NaN  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(run_results, orient=\"index\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m text_match_df = df.copy()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m text_match_df.columns:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     text_match_df[column] = \u001b[43mtext_match_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_matches\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m text_match_df\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manim\\ASU\\Legal-Document-Discrepancy-Benchmark-Dataset\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manim\\ASU\\Legal-Document-Discrepancy-Benchmark-Dataset\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manim\\ASU\\Legal-Document-Discrepancy-Benchmark-Dataset\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manim\\ASU\\Legal-Document-Discrepancy-Benchmark-Dataset\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\manim\\ASU\\Legal-Document-Discrepancy-Benchmark-Dataset\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      1\u001b[39m text_match_df = df.copy()\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m text_match_df.columns:\n\u001b[32m      3\u001b[39m     text_match_df[column] = text_match_df[column].apply(\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m\"\u001b[39m\u001b[33mtext_matches\u001b[39m\u001b[33m\"\u001b[39m] / x[\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtotal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      6\u001b[39m text_match_df\n",
      "\u001b[31mTypeError\u001b[39m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "text_match_df = df.copy()\n",
    "for column in text_match_df.columns:\n",
    "    text_match_df[column] = text_match_df[column].apply(\n",
    "        lambda x: x[\"text_matches\"] / x[\"total\"] if x[\"total\"] > 0 else 0\n",
    "    )\n",
    "text_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ambiguity_inText</th>\n",
       "      <th>inconsistencies_inText</th>\n",
       "      <th>misaligned_terminalogy_inText</th>\n",
       "      <th>omissions_inText</th>\n",
       "      <th>structural_flaws_inText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>zero-shot-self-verification</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot-self-verification-cot</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot-self-verification</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot-cot</th>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>few-shot-self-verification-cot</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot-cot</th>\n",
       "      <td>0.144737</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zero-shot</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ambiguity_inText  inconsistencies_inText  \\\n",
       "zero-shot-self-verification              0.000000                0.200000   \n",
       "zero-shot-self-verification-cot          0.000000                0.133333   \n",
       "few-shot                                 0.333333                0.333333   \n",
       "few-shot-self-verification               0.466667                0.200000   \n",
       "few-shot-cot                             0.466667                0.312500   \n",
       "few-shot-self-verification-cot           0.285714                0.266667   \n",
       "zero-shot-cot                            0.144737                0.226667   \n",
       "zero-shot                                0.047619                0.210526   \n",
       "\n",
       "                                 misaligned_terminalogy_inText  \\\n",
       "zero-shot-self-verification                           0.066667   \n",
       "zero-shot-self-verification-cot                       0.133333   \n",
       "few-shot                                              0.200000   \n",
       "few-shot-self-verification                            0.333333   \n",
       "few-shot-cot                                          0.466667   \n",
       "few-shot-self-verification-cot                        0.333333   \n",
       "zero-shot-cot                                         0.133333   \n",
       "zero-shot                                             0.157895   \n",
       "\n",
       "                                 omissions_inText  structural_flaws_inText  \n",
       "zero-shot-self-verification              0.133333                 0.266667  \n",
       "zero-shot-self-verification-cot          0.066667                 0.400000  \n",
       "few-shot                                 0.133333                 0.400000  \n",
       "few-shot-self-verification               0.117647                 0.312500  \n",
       "few-shot-cot                             0.117647                 0.235294  \n",
       "few-shot-self-verification-cot           0.176471                 0.625000  \n",
       "zero-shot-cot                            0.106667                 0.266667  \n",
       "zero-shot                                0.100000                 0.136364  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_match_df = df.copy()\n",
    "for column in text_match_df.columns:\n",
    "    text_match_df[column] = text_match_df[column].apply(\n",
    "        lambda x: x[\"correct\"] / x[\"total\"] if x[\"total\"] > 0 else 0\n",
    "    )\n",
    "text_match_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zero-shot-self-verification        0.133333\n",
       "zero-shot-self-verification-cot    0.146667\n",
       "few-shot                           0.280000\n",
       "few-shot-self-verification         0.282051\n",
       "few-shot-cot                       0.312500\n",
       "few-shot-self-verification-cot     0.337662\n",
       "zero-shot-cot                      0.158228\n",
       "zero-shot                          0.128713\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_correct_score(row):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for col in row.index:\n",
    "        total += row[col][\"total\"]\n",
    "        correct += row[col][\"correct\"]\n",
    "    return correct / total if total > 0 else 0\n",
    "        \n",
    "# Text Match\n",
    "total_score = df.copy()\n",
    "total_score.apply(aggregate_correct_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zero-shot-self-verification        0.320000\n",
       "zero-shot-self-verification-cot    0.293333\n",
       "few-shot                           0.613333\n",
       "few-shot-self-verification         0.564103\n",
       "few-shot-cot                       0.537500\n",
       "few-shot-self-verification-cot     0.545455\n",
       "zero-shot-cot                      0.205696\n",
       "zero-shot                          0.554455\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_correct_score(row):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for col in row.index:\n",
    "        total += row[col][\"total\"]\n",
    "        correct += row[col][\"text_matches\"]\n",
    "    return correct / total if total > 0 else 0\n",
    "        \n",
    "# Text Match\n",
    "total_score = df.copy()\n",
    "total_score.apply(aggregate_correct_score, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few-shot variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO \n",
    "---\n",
    "- Z ✅\n",
    "- Z + COT ✅\n",
    "- Z + SV ✅\n",
    "- Z + COT + SV ✅\n",
    "- Z + SC ✅\n",
    "- Z + COT + SC ✅\n",
    "---\n",
    "- FS ✅⚠️\n",
    "- FS + COT ✅⚠️\n",
    "- FS + SV ✅⚠️\n",
    "- FS + COT + SV ✅⚠️\n",
    "- FS + SC ✅⚠️\n",
    "- FS + COT + SC ✅⚠️\n",
    "---\n",
    "- Z + SV + SC (SKIP THIS FOR NOW) ✅\n",
    "- Z + COT + SV + SC (SKIP THIS FOR NOW) ✅\n",
    "- FS + SV + SC (SKIP THIS FOR NOW) ✅⚠️\n",
    "- FS + COT + SV + SC (SKIP THIS FOR NOW) ✅⚠️\n",
    "---\n",
    "- **Output into a .csv**❌\n",
    "- **Eventually need to repeat with different LLMs**❌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "1) `text match` but `explanation !match` = -1\n",
    "2) `text match` and `explanation match` = +1\n",
    "3) `text !match` and `explanation match` = -1\n",
    "4) `text !match` and `explanation !match` = -1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
