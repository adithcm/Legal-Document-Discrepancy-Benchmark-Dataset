{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, time, pathlib, textwrap, itertools, contextlib\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "import csv, itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf910f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── EDIT HERE if you ever move the folders ────────────────────────────\n",
    "JSON_ROOT   = pathlib.Path(\"Legal_doc_test\")   # ← your master folder\n",
    "OUT_DIR     = pathlib.Path(\"scraped_laws\")     # snippets will be saved here\n",
    "CHARS_AROUND = 1000                             # how many characters to keep\n",
    "#REQUEST_TIMEOUT = 15000                           # seconds\n",
    "# ────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Searching for JSONs under:\", JSON_ROOT.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_snippet(url: str, max_chars: int = 10000) -> Optional[str]:\n",
    "    \"\"\"Download `url` and return the first `max_chars` of visible text, or None on failure.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            timeout=REQUEST_TIMEOUT,\n",
    "            headers={\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "    except Exception as exc:                # network or HTTP error\n",
    "        print(f\"  ⚠️  request failed: {exc}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    # strip non‑content tags\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)        # collapse whitespace\n",
    "    return text[:max_chars] or None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the first JSON we can find\n",
    "test_json = next(JSON_ROOT.rglob(\"*.json\"))\n",
    "print(\"Testing on:\", test_json.relative_to(JSON_ROOT))\n",
    "\n",
    "with open(test_json, encoding=\"utf-8\") as fh:\n",
    "    sample = json.load(fh)[0]               # JSONs are wrapped in a list\n",
    "first_law = sample[\"perturbation\"][0]\n",
    "\n",
    "snippet = fetch_snippet(first_law[\"law_url\"], CHARS_AROUND)\n",
    "print(\"\\n— Scraped snippet —\\n\")\n",
    "print(textwrap.fill(snippet or \"⟨nothing scraped⟩\", width=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8023843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_files(root: pathlib.Path):\n",
    "    yield from root.rglob(\"*.json\")\n",
    "\n",
    "def target_path(json_path: pathlib.Path) -> pathlib.Path:\n",
    "    return OUT_DIR / json_path.relative_to(JSON_ROOT).with_suffix(\".snippet.json\")\n",
    "\n",
    "# open the log file once; append mode so successive runs accumulate\n",
    "log_path = OUT_DIR / \"scrape_log.tsv\"\n",
    "log_fh   = open(log_path, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "log      = csv.writer(log_fh, delimiter=\"\\t\")\n",
    "if log_fh.tell() == 0:          # header only the first time\n",
    "    log.writerow([\"timestamp\", \"json_file\", \"law_url\", \"status\"])\n",
    "\n",
    "errors = []\n",
    "\n",
    "def note(json_file, url, status):\n",
    "    \"\"\"Write one line to log file and echo to notebook.\"\"\"\n",
    "    log.writerow([datetime.utcnow().isoformat(), json_file, url, status])\n",
    "    print(f\"{json_file}  ->  {status}\")\n",
    "\n",
    "session = requests.Session()    # reuse TCP connection\n",
    "\n",
    "def fetch_snippet_with_retry(url, max_chars=800):\n",
    "    # Try twice with different headers (some sites dislike 'python-requests')\n",
    "    hdr_sets = [\n",
    "        {\"User-Agent\": \"Mozilla/5.0\"},                           # normal browser UA\n",
    "        {\"User-Agent\": \"curl/8.5.0\"}                             # plain CLI UA\n",
    "    ]\n",
    "    for hdr in hdr_sets:\n",
    "        try:\n",
    "            r = session.get(url, timeout=REQUEST_TIMEOUT, headers=hdr)\n",
    "            r.raise_for_status()\n",
    "            # success – hand over to BeautifulSoup cleaner\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                tag.decompose()\n",
    "            txt = re.sub(r\"\\s+\", \" \", soup.get_text(\" \", strip=True))\n",
    "            return txt[:max_chars] or None\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "    raise last_exc   # both attempts failed\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "for jpath in tqdm(list(all_json_files(JSON_ROOT)), desc=\"scraping\"):\n",
    "    out_path = target_path(jpath)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        continue     # already scraped\n",
    "\n",
    "    try:\n",
    "        data = json.loads(jpath.read_text(encoding=\"utf-8\"))\n",
    "        for pert in data[0][\"perturbation\"]:\n",
    "            url = (pert.get(\"law_url\") or \"\").strip()\n",
    "            if not url or url.lower().startswith(\"n/a\"):\n",
    "                pert[\"scraped_snippet\"] = None\n",
    "                note(jpath.relative_to(JSON_ROOT), url or \"∅\", \"SKIPPED (no url)\")\n",
    "                continue\n",
    "            try:\n",
    "                pert[\"scraped_snippet\"] = fetch_snippet_with_retry(url, CHARS_AROUND)\n",
    "                status = \"OK\" if pert[\"scraped_snippet\"] else \"EMPTY\"\n",
    "                note(jpath.relative_to(JSON_ROOT), url, status)\n",
    "            except Exception as exc:\n",
    "                pert[\"scraped_snippet\"] = None\n",
    "                status = f\"ERROR: {type(exc).__name__} – {exc}\"\n",
    "                note(jpath.relative_to(JSON_ROOT), url, status)\n",
    "                errors.append((jpath, url, str(exc)))\n",
    "            time.sleep(0.5)  # be polite\n",
    "\n",
    "        out_path.write_text(json.dumps(data, indent=2, ensure_ascii=False),\n",
    "                            encoding=\"utf-8\")\n",
    "\n",
    "    except Exception as exc:\n",
    "        errors.append((jpath, \"⟨parsing⟩\", str(exc)))\n",
    "        note(jpath.relative_to(JSON_ROOT), \"⟨parsing⟩\",\n",
    "             f\"ERROR: {type(exc).__name__} – {exc}\")\n",
    "\n",
    "log_fh.close()\n",
    "\n",
    "print(f\"\\nFinished.  {len(errors)} issues logged to {log_path.relative_to(OUT_DIR)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
