{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bb35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- standard libs --------------------\n",
    "import os, re, json, csv, time, pathlib, hashlib, textwrap, contextlib\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "# -------------------- third‑party ----------------------\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -------------------- paths & constants ----------------\n",
    "JSON_ROOT     = pathlib.Path(\"Legal_doc_test\")          # source JSONs\n",
    "OUT_DIR       = pathlib.Path(\"scraped_laws_v7\")            # where snippets + log go\n",
    "CACHE_DIR     = OUT_DIR / \".cache\"                      # html cache (optional)\n",
    "\n",
    "CHARS_AROUND  = 2000                                     # context around § hit\n",
    "REQUEST_TIMEOUT = 15                                    # seconds\n",
    "SLEEP_BETWEEN = 1.0                                     # polite pause\n",
    "\n",
    "# ensure dirs exist\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"JSON root :\", JSON_ROOT.resolve())\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf910f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "UA_STRINGS = [\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "    \"curl/8.5.0\",\n",
    "    \"python-requests/2.31\"\n",
    "]\n",
    "\n",
    "session = requests.Session()          # TCP reuse\n",
    "\n",
    "\n",
    "def resilient_get(url: str, ua: str) -> str:\n",
    "    \"\"\"\n",
    "    Download `url` with the given User‑Agent.\n",
    "    Falls back to Google web‑cache if 404/410.\n",
    "    \"\"\"\n",
    "    hdr = {\"User-Agent\": ua, \"Referer\": \"https://google.com\"}\n",
    "    try:\n",
    "        r = session.get(url, timeout=REQUEST_TIMEOUT, headers=hdr)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except requests.HTTPError as e:\n",
    "        if e.response.status_code in (404, 410):\n",
    "            cache_url = f\"https://webcache.googleusercontent.com/search?q={url}\"\n",
    "            r = session.get(cache_url, timeout=REQUEST_TIMEOUT, headers=hdr)\n",
    "            r.raise_for_status()\n",
    "            return r.text\n",
    "        raise\n",
    "\n",
    "\n",
    "def extract_relevant_section(text: str, citation: str,\n",
    "                             context: int = CHARS_AROUND) -> str:\n",
    "    \"\"\"\n",
    "    Return ~`context` chars before/after the first match for the section number\n",
    "    found in `citation` (e.g. '151.002').  Fallback: first 2*context chars.\n",
    "    \"\"\"\n",
    "    # pull the numeric part (151.002, 2‑306, etc.)\n",
    "    m_sec = re.search(r'(\\d+\\.\\d+)', citation)\n",
    "    section_id = m_sec.group(1) if m_sec else None\n",
    "\n",
    "    if section_id:\n",
    "        m = re.search(rf'\\b{re.escape(section_id)}\\b', text)\n",
    "        if m:\n",
    "            start = max(0, m.start() - context)\n",
    "            end   = m.end() + context\n",
    "            return text[start:end]\n",
    "\n",
    "    # fallback – nothing matched\n",
    "    return text[:context * 2]\n",
    "\n",
    "\n",
    "def fetch_snippet_with_retry(url: str, citation: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Try each UA in UA_STRINGS.  Clean HTML → plain text →\n",
    "    isolate relevant section → return snippet (or None).\n",
    "    \"\"\"\n",
    "    for ua in UA_STRINGS:\n",
    "        try:\n",
    "            # ---------- caching ----------\n",
    "            key = CACHE_DIR / hashlib.sha1(f\"{ua}|{url}\".encode()).hexdigest()\n",
    "            if key.exists():\n",
    "                html = key.read_text(encoding=\"utf-8\")\n",
    "            else:\n",
    "                html = resilient_get(url, ua)\n",
    "                key.write_text(html, encoding=\"utf-8\")\n",
    "\n",
    "            # ---------- clean ----------\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
    "                tag.decompose()\n",
    "            text = re.sub(r\"\\s+\", \" \", soup.get_text(\" \", strip=True))\n",
    "            text = text.replace(\"\\ufeff\", \"\")            # BOM\n",
    "\n",
    "            # ---------- slice ----------\n",
    "            return extract_relevant_section(text, citation)\n",
    "\n",
    "        except Exception:\n",
    "            continue    # try next UA\n",
    "\n",
    "    return None          # all attempts failed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_json_files(root: pathlib.Path):\n",
    "    \"\"\"Yield every *.json under `root`, skipping .ipynb_checkpoints.\"\"\"\n",
    "    for p in root.rglob(\"*.json\"):\n",
    "        if \".ipynb_checkpoints\" in p.parts:\n",
    "            continue\n",
    "        yield p\n",
    "\n",
    "\n",
    "def target_path(json_path: pathlib.Path) -> pathlib.Path:\n",
    "    \"\"\"Mirror directory tree under OUT_DIR with *.snippet.json suffix.\"\"\"\n",
    "    return OUT_DIR / json_path.relative_to(JSON_ROOT).with_suffix(\".snippet.json\")\n",
    "\n",
    "\n",
    "# ------------- TSV log -------------\n",
    "log_path = OUT_DIR / \"scrape_log.tsv\"\n",
    "log_fh   = open(log_path, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "log      = csv.writer(log_fh, delimiter=\"\\t\")\n",
    "\n",
    "if log_fh.tell() == 0:          # header only once\n",
    "    log.writerow([\"timestamp\", \"json_file\", \"law_url\", \"status\", \"chars\"])\n",
    "\n",
    "def note(jfile, url, status, chars=0):\n",
    "    log.writerow([datetime.utcnow().isoformat(), jfile, url, status, chars])\n",
    "    print(f\"{jfile:80s}  ->  {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d64e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "\n",
    "for jpath in tqdm(list(all_json_files(JSON_ROOT)), desc=\"scraping\"):\n",
    "    out_path = target_path(jpath)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.exists():\n",
    "        continue                               # already processed\n",
    "\n",
    "    try:\n",
    "        data = json.loads(jpath.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "        for pert in data[0][\"perturbation\"]:\n",
    "            url  = (pert.get(\"law_url\") or \"\").strip()\n",
    "            cite = (pert.get(\"law_citation\") or \"\").strip()\n",
    "\n",
    "            if not url or url.lower().startswith(\"n/a\"):\n",
    "                pert[\"scraped_snippet\"] = None\n",
    "                note(jpath.name, url or \"∅\", \"SKIPPED (no url)\")\n",
    "                continue\n",
    "\n",
    "            snippet = fetch_snippet_with_retry(url, cite)\n",
    "            pert[\"scraped_snippet\"] = snippet\n",
    "            status = \"OK\" if snippet else \"EMPTY\"\n",
    "            note(jpath.name, url, status, len(snippet or \"\"))\n",
    "\n",
    "            time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "        out_path.write_text(json.dumps(data, indent=2, ensure_ascii=False),\n",
    "                            encoding=\"utf-8\")\n",
    "\n",
    "    except Exception as exc:\n",
    "        errors.append((jpath, str(exc)))\n",
    "        note(jpath.name, \"⟨parsing⟩\", f\"ERROR: {exc}\")\n",
    "\n",
    "log_fh.close()\n",
    "print(f\"\\nFinished.  {len(errors)} errors logged → {log_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8023843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display one example snippet nicely\n",
    "example_json = next(OUT_DIR.rglob(\"*.snippet.json\"))\n",
    "example = json.loads(example_json.read_text(encoding=\"utf-8\"))[0]\n",
    "first = example[\"perturbation\"][0]\n",
    "\n",
    "print(\"→\", example_json.relative_to(OUT_DIR))\n",
    "print(\"Law URL   :\", first[\"law_url\"])\n",
    "print(\"Citation  :\", first[\"law_citation\"])\n",
    "print(\"\\nSnippet   :\\n\")\n",
    "print(textwrap.fill(first[\"scraped_snippet\"] or \"⟨nothing⟩\", width=90))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
